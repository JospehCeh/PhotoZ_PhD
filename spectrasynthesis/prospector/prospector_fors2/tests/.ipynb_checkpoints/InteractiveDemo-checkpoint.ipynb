{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Demo\n",
    "This is a deconstructed version of the demo that is intended to show a bit more detail about the operation of *prospector*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's set up some environmental dependencies. These just make the numerics easier and adjust some of the plotting defaults to make things more legible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, sys, os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy\n",
    "from matplotlib.pyplot import *\n",
    "import re\n",
    "import h5py\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from collections import OrderedDict\n",
    "\n",
    "# re-defining plotting defaults\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib import gridspec\n",
    "rcParams.update({'xtick.major.pad': '7.0'})\n",
    "rcParams.update({'xtick.major.size': '7.5'})\n",
    "rcParams.update({'xtick.major.width': '1.5'})\n",
    "rcParams.update({'xtick.minor.pad': '7.0'})\n",
    "rcParams.update({'xtick.minor.size': '3.5'})\n",
    "rcParams.update({'xtick.minor.width': '1.0'})\n",
    "rcParams.update({'ytick.major.pad': '7.0'})\n",
    "rcParams.update({'ytick.major.size': '7.5'})\n",
    "rcParams.update({'ytick.major.width': '1.5'})\n",
    "rcParams.update({'ytick.minor.pad': '7.0'})\n",
    "rcParams.update({'ytick.minor.size': '3.5'})\n",
    "rcParams.update({'ytick.minor.width': '1.0'})\n",
    "rcParams.update({'xtick.color': 'k'})\n",
    "rcParams.update({'ytick.color': 'k'})\n",
    "rcParams.update({'font.size': 30})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor, kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prospector** utilizes three main packages:\n",
    "- **fsps**, which governs the fundamental stellar population synthesis models (via the **python-fsps** package),\n",
    "- **sedpy**, which contains some routines for computing projecting spectra onto filter bandpasses, and\n",
    "- **prospect**, which is where the likelihood evaluations, parameter priors, and posterior sampling takes place.\n",
    "\n",
    "Let's import those now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsps\n",
    "import sedpy\n",
    "import prospect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's just look at the versions so you know what's working for me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy: 1.23.5\n",
      "scipy: 1.10.1\n",
      "h5py: 3.7.0\n",
      "fsps: 0.4.3\n",
      "prospect: 1.2.1.dev23+g09a83f2\n"
     ]
    }
   ],
   "source": [
    "vers = (np.__version__, scipy.__version__, h5py.__version__, fsps.__version__, prospect.__version__)\n",
    "print(\"numpy: {}\\nscipy: {}\\nh5py: {}\\nfsps: {}\\nprospect: {}\".format(*vers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also use external packages for sampling the posterior.  Let's make sure we have those now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "import dynesty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_keys = ['name','num','ra','dec', 'redshift','Rmag','RT', 'RV','eRV','Nsp','lines',\n",
    "                'ra_galex','dec_galex','fuv_mag', 'fuv_magerr','nuv_mag', 'nuv_magerr', \n",
    "                'fuv_flux', 'fuv_fluxerr','nuv_flux', 'nuv_fluxerr','asep_galex',\n",
    "                'ID', 'KIDS_TILE','RAJ2000','DECJ2000','Z_ML', 'Z_B','asep_kids','CLASS_STAR',\n",
    "                'MAG_GAAP_u','MAG_GAAP_g','MAG_GAAP_r','MAG_GAAP_i','MAG_GAAP_Z','MAG_GAAP_Y','MAG_GAAP_J', 'MAG_GAAP_H','MAG_GAAP_Ks',\n",
    "                'MAGERR_GAAP_u','MAGERR_GAAP_g','MAGERR_GAAP_r','MAGERR_GAAP_i','MAGERR_GAAP_Z','MAGERR_GAAP_Y','MAGERR_GAAP_J','MAGERR_GAAP_H','MAGERR_GAAP_Ks',\n",
    "                'FLUX_GAAP_u','FLUX_GAAP_g','FLUX_GAAP_r','FLUX_GAAP_i','FLUX_GAAP_Z','FLUX_GAAP_Y','FLUX_GAAP_J', 'FLUX_GAAP_H','FLUX_GAAP_Ks',\n",
    "                'FLUXERR_GAAP_u','FLUXERR_GAAP_g','FLUXERR_GAAP_r','FLUXERR_GAAP_i','FLUXERR_GAAP_Z','FLUXERR_GAAP_Y','FLUXERR_GAAP_J','FLUXERR_GAAP_H','FLUXERR_GAAP_Ks',\n",
    "                'FLUX_RADIUS', 'EXTINCTION_u','EXTINCTION_g','EXTINCTION_r', 'EXTINCTION_i',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fors2DataAcess(object):\n",
    "    def __init__(self,filename):\n",
    "        if os.path.isfile(filename):\n",
    "            self.hf = h5py.File(filename, 'r')\n",
    "            self.list_of_groupkeys = list(self.hf.keys())      \n",
    "             # pick one key    \n",
    "            key_sel =  self.list_of_groupkeys[0]\n",
    "            # pick one group\n",
    "            group = self.hf.get(key_sel)  \n",
    "            #pickup all attribute names\n",
    "            self.list_of_subgroup_keys = []\n",
    "            for k in group.attrs.keys():\n",
    "                self.list_of_subgroup_keys.append(k)\n",
    "        else:\n",
    "            self.hf = None\n",
    "            self.list_of_groupkeys = []\n",
    "            self.list_of_subgroup_keys = []\n",
    "    def close_file(self):\n",
    "        self.hf.close() \n",
    "        \n",
    "    def get_list_of_groupkeys(self):\n",
    "        return self.list_of_groupkeys \n",
    "    def get_list_subgroup_keys(self):\n",
    "        return self.list_of_subgroup_keys\n",
    "    def getattribdata_fromgroup(self,groupname):\n",
    "        attr_dict = OrderedDict()\n",
    "        if groupname in self.list_of_groupkeys:       \n",
    "            group = self.hf.get(groupname)  \n",
    "            for  nameval in self.list_of_subgroup_keys:\n",
    "                attr_dict[nameval] = group.attrs[nameval]\n",
    "        else:\n",
    "            print(f'getattribdata_fromgroup : No group {groupname}')\n",
    "        return attr_dict\n",
    "    def getspectrum_fromgroup(self,groupname):\n",
    "        spec_dict = {}\n",
    "        if groupname in self.list_of_groupkeys:       \n",
    "            group = self.hf.get(groupname)  \n",
    "            wl = np.array(group.get(\"wl\"))\n",
    "            fl = np.array(group.get(\"fl\")) \n",
    "            spec_dict[\"wl\"] = wl\n",
    "            spec_dict[\"fl\"] = fl\n",
    "        else:\n",
    "            print(f'getspectrum_fromgroup : No group {groupname}')\n",
    "        return spec_dict\n",
    "    \n",
    "    #kernel = kernels.RBF(0.5, (8000, 10000.0))\n",
    "    #gp = GaussianProcessRegressor(kernel=kernel ,random_state=0)\n",
    "\n",
    "    def getspectrumcleanedemissionlines_fromgroup(self,groupname,gp,nsigs=8):\n",
    "        spec_dict = {}\n",
    "        if groupname in self.list_of_groupkeys:       \n",
    "            group = self.hf.get(groupname)  \n",
    "            wl = np.array(group.get(\"wl\"))\n",
    "            fl = np.array(group.get(\"fl\")) \n",
    "            # fit gaussian pricess \n",
    "            X = wl\n",
    "            Y = fl\n",
    "            \n",
    "            DeltaY = Y - gp.predict(X[:, None], return_std=False)\n",
    "            background = np.sqrt(np.median(DeltaY**2))\n",
    "            indexes_toremove = np.where(np.abs(DeltaY)> nsigs * background)[0]\n",
    "            \n",
    "            Xclean = np.delete(X,indexes_toremove)\n",
    "            Yclean  = np.delete(Y,indexes_toremove)\n",
    "            \n",
    "            spec_dict[\"wl\"] = Xclean\n",
    "            spec_dict[\"fl\"] = Yclean\n",
    "    \n",
    "            \n",
    "        else:\n",
    "            print(f'getspectrum_fromgroup : No group {groupname}')\n",
    "        return spec_dict\n",
    "        \n",
    "        \n",
    "           \n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Prospector* requires several ingredients to conduct a fit.  These are:\n",
    "  1. An `obs` dictionary (with the data we intend to fit)\n",
    "  2. A stellar population synthesis object (a *source*) to predict spectra from parameters\n",
    "  3. A `model` object (to describe, store, and translate parameters and priors)\n",
    "  4. A likelihood or posterior probability function\n",
    "  \n",
    "It can also useful to collect the meta-parameters controlling how these ingredients are created and how the fit is conducted in a ``run_params`` dictionary. We will do that as we go along.  We will also encapsulate each step of the setup in a series of `build_x()` functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data to be fit: `build_obs()`\n",
    "\n",
    "The data to be fit is stored in a dictionary that we will call `obs`.\n",
    "The `obs` dictionary stores the observed fluxes, uncertainties, and information about how those fluxes were measured.\n",
    "For spectra this means the wavelengths at which the fluxes were measured, while for photometry we must know the filters through which the fluxes were measured.\n",
    "This information is passed (via the `obs` dictionary) to the model object to specify which data to predict.  It is also passed to the likelihood functions for calculating the likelihood of the data for a given model.\n",
    "\n",
    "**Units:** The units of the fluxes and uncertainties are assumed to be maggies (i.e Jy/3631).\n",
    "The wavelength units are assumed to be observed frame vacuum angstroms.  Also note that mask values should be `True` for data that you want to fit, and `False` for data that is to be ignored in the likelihood calculation.\n",
    "\n",
    "We will create an `obs` dictionary below, with all of the required keys, starting with photometry.\n",
    "We will do this through a `build_obs` method, that takes some meta-parameters.\n",
    "In this example we use photometry from a row in Table 2 of Johnson et al. 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_h5  = '../../../../QueryCatalogs/data/FORS2spectraGalexKidsPhotom.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fors2 = Fors2DataAcess(input_file_h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_keys = fors2.get_list_of_groupkeys()\n",
    "list_of_attributes = fors2.get_list_subgroup_keys()\n",
    "list_of_keys = np.array(list_of_keys)\n",
    "list_of_keysnum = [ int(re.findall(\"SPEC(.*)\",specname)[0]) for specname in  list_of_keys ]\n",
    "sorted_indexes = np.argsort(list_of_keysnum)\n",
    "list_of_keys = list_of_keys[sorted_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = pd.DataFrame(columns=list_of_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = kernels.RBF(0.5, (8000, 10000.0))\n",
    "gp = GaussianProcessRegressor(kernel=kernel ,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seleced_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m selected_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSPEC214\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m output_tag \u001b[38;5;241m=\u001b[39m \u001b[43mseleced_key\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_photomonly\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seleced_key' is not defined"
     ]
    }
   ],
   "source": [
    "selected_key = 'SPEC214'\n",
    "output_tag = selected_key + '_photomonly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = fors2.getattribdata_fromgroup(selected_key)\n",
    "spectr = fors2.getspectrumcleanedemissionlines_fromgroup(selected_key,gp)\n",
    "df_info.loc[0] = [*attrs.values()] # hope the order of attributes is kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = df_info[ordered_keys]\n",
    "df_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_redshift = df_info[\"redshift\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_obs(attrs, spectr, **extras):\n",
    "    \"\"\"Build a dictionary of observational data.  \n",
    "        into apparent magnitudes.\n",
    "        \n",
    "    :returns obs:\n",
    "        A dictionary of observational data to use in the fit.\n",
    "    \"\"\"\n",
    "    from prospect.utils.obsutils import fix_obs\n",
    "    import sedpy\n",
    "\n",
    "    # The obs dictionary, empty for now\n",
    "    obs = {}\n",
    "\n",
    "    # These are the names of the relevant filters, \n",
    "    # in the same order as the photometric data (see below)\n",
    "    galex = ['galex_FUV', 'galex_NUV']\n",
    "    vista = ['vista_vircam_'+n for n in ['Z','J','H','Ks']]\n",
    "    sdss = ['sdss_{0}0'.format(b) for b in ['u','g','r','i']]\n",
    "    filternames = galex + sdss + vista\n",
    "    # And here we instantiate the `Filter()` objects using methods in `sedpy`,\n",
    "    # and put the resultinf list of Filter objects in the \"filters\" key of the `obs` dictionary\n",
    "    obs[\"filters\"] = sedpy.observate.load_filters(filternames)\n",
    "  \n",
    "    # Now we store the measured fluxes for a single object, **in the same order as \"filters\"**\n",
    "    # In this example we use a row of absolute AB magnitudes from Johnson et al. 2013 (NGC4163)\n",
    "    # We then turn them into apparent magnitudes based on the supplied `ldist` meta-parameter.\n",
    "    # You could also, e.g. read from a catalog.\n",
    "    # The units of the fluxes need to be maggies (Jy/3631) so we will do the conversion here too.\n",
    "    m_AB = np.array([attrs['fuv_mag'], attrs['nuv_mag'],\n",
    "                     attrs['MAG_GAAP_u'],attrs['MAG_GAAP_g'],attrs['MAG_GAAP_r'],attrs['MAG_GAAP_i'],\n",
    "                     attrs['MAG_GAAP_Z'],attrs['MAG_GAAP_J'],attrs['MAG_GAAP_H'],attrs['MAG_GAAP_Ks']])\n",
    "    merr = np.array([attrs['fuv_magerr'], attrs['nuv_magerr'],\n",
    "                     attrs['MAGERR_GAAP_u'],attrs['MAGERR_GAAP_g'],attrs['MAGERR_GAAP_r'],attrs['MAGERR_GAAP_i'],\n",
    "                     attrs['MAGERR_GAAP_Z'],attrs['MAGERR_GAAP_J'],attrs['MAGERR_GAAP_H'],attrs['MAGERR_GAAP_Ks']])               \n",
    "    \n",
    "    mags = m_AB\n",
    "    obs[\"maggies\"] = 10**(-0.4*mags)\n",
    "    \n",
    "    # And now we store the uncertainties (again in units of maggies)\n",
    "    # In this example we are going to fudge the uncertainties based on the supplied `snr` meta-parameter.\n",
    "    obs['maggies_unc'] = np.array([0.9210*merr[i]* obs[\"maggies\"][i] for i in range(len(mags))])\n",
    "    \n",
    "    \n",
    "    # Now we need a mask, which says which flux values to consider in the likelihood.\n",
    "    # IMPORTANT: the mask is *True* for values that you *want* to fit, \n",
    "    # and *False* for values you want to ignore.  Here we ignore the spitzer bands.\n",
    "    obs[\"phot_mask\"] = np.array([ ~np.isnan(f) for f in obs[\"maggies\"]])\n",
    "\n",
    "    # This is an array of effective wavelengths for each of the filters.  \n",
    "    # It is not necessary, but it can be useful for plotting so we store it here as a convenience\n",
    "    obs[\"phot_wave\"] = np.array([f.wave_effective for f in obs[\"filters\"]])\n",
    "    \n",
    "\n",
    "    # We do not have a spectrum, so we set some required elements of the obs dictionary to None.\n",
    "    # (this would be a vector of vacuum wavelengths in angstroms)\n",
    "    obs[\"wavelength\"] = None\n",
    "    # (this would be the spectrum in units of maggies)\n",
    "    obs[\"spectrum\"] = None\n",
    "    # (spectral uncertainties are given here)\n",
    "    obs['unc'] = None\n",
    "    # (again, to ignore a particular wavelength set the value of the \n",
    "    #  corresponding elemnt of the mask to *False*)\n",
    "    obs['mask'] = None\n",
    "\n",
    "\n",
    "    # This function ensures all required keys are present in the obs dictionary,\n",
    "    # adding default values if necessary\n",
    "    obs = fix_obs(obs)\n",
    "\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will store some meta-parameters that control the input arguments to this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params = {}\n",
    "run_params[\"snr\"] = 10.0\n",
    "run_params[\"ldist\"] = 10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the final `obs` disctionary and also plot up our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the obs dictionary using the meta-parameters\n",
    "obs = build_obs(attrs, spectr,**run_params)\n",
    "\n",
    "# Look at the contents of the obs dictionary\n",
    "print(\"Obs Dictionary Keys:\\n\\n{}\\n\".format(obs.keys()))\n",
    "print(\"--------\\nFilter objects:\\n\")\n",
    "print(obs[\"filters\"])\n",
    "\n",
    "# --- Plot the Data ----\n",
    "# This is why we stored these...\n",
    "wphot = obs[\"phot_wave\"]\n",
    "\n",
    "# establish bounds\n",
    "xmin, xmax = np.min(wphot)*0.8, np.max(wphot)/0.8\n",
    "ymin, ymax = obs[\"maggies\"][1:].min()*0.8, obs[\"maggies\"][1:].max()/0.4\n",
    "figure(figsize=(16,8))\n",
    "\n",
    "# plot all the data\n",
    "plot(wphot, obs['maggies'],\n",
    "     label='All observed photometry',\n",
    "     marker='o', markersize=12, alpha=0.8, ls='', lw=3,\n",
    "     color='slateblue')\n",
    "\n",
    "# overplot only the data we intend to fit\n",
    "mask = obs[\"phot_mask\"]\n",
    "errorbar(wphot[mask], obs['maggies'][mask], \n",
    "         yerr=obs['maggies_unc'][mask], \n",
    "         label='Photometry to fit',\n",
    "         marker='o', markersize=8, alpha=0.8, ls='', lw=3,\n",
    "         ecolor='tomato', markerfacecolor='none', markeredgecolor='tomato', \n",
    "         markeredgewidth=3)\n",
    "\n",
    "# plot Filters\n",
    "for f in obs['filters']:\n",
    "    w, t = f.wavelength.copy(), f.transmission.copy()\n",
    "    t = t / t.max()\n",
    "    t = 10**(0.2*(np.log10(ymax/ymin)))*t * ymin\n",
    "    loglog(w, t, lw=3, color='gray', alpha=0.7)\n",
    "\n",
    "# prettify\n",
    "xlabel('Wavelength [A]')\n",
    "ylabel('Flux Density [maggies]')\n",
    "xlim([xmin, xmax])\n",
    "ylim([ymin, ymax])\n",
    "xscale(\"log\")\n",
    "yscale(\"log\")\n",
    "legend(loc='best', fontsize=20)\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model object: `build_model()`\n",
    "\n",
    "Now we need a set of model parameters, which will **define** the model we are tying to fit to the data.  The model object stores the parameters that are used by the SPS object to build a spectrum, as well as infomation about which parameters are to be varied during fitting, and priors on those parameters.  It efficiently converts between a vector of parameter values (the `theta` attribute) used by the MCMC samplers or optimizers and the dictionary of parameter names and values (the `params` attribute) that can be passed to the sps objects' `get_spectrum()` method.\n",
    "\n",
    "To create the model object we need a list or dictionary of model parameters and some infomation about them.  Each parameter must a have a `name`, a length `N` (vector parameters are possible), an initial value `ini`, and must be specified as either a free parameter or a fixed parameter via an `isfree` key.  If it is a free parameter it *must* have a prior specified as well, which we will get from the `priors` module.  Please see the documentation for that module for detaiuls on the available priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a dictionary that describes a single parameter controlling the stellar mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prospect.models import priors\n",
    "mass_param = {\"name\": \"mass\",\n",
    "              # The mass parameter here is a scalar, so it has N=1\n",
    "              \"N\": 1,\n",
    "              # We will be fitting for the mass, so it is a free parameter\n",
    "              \"isfree\": True,\n",
    "              # This is the initial value. For fixed parameters this is the\n",
    "              # value that will always be used. \n",
    "              \"init\": 1e8,\n",
    "              # This sets the prior probability for the parameter\n",
    "              \"prior\": priors.LogUniform(mini=1e6, maxi=1e12),\n",
    "              # this sets the initial dispersion to use when generating \n",
    "              # clouds of emcee \"walkers\".  It is not required, but can be very helpful.\n",
    "              \"init_disp\": 1e6, \n",
    "              # this sets the minimum dispersion to use when generating \n",
    "              #clouds of emcee \"walkers\".  It is not required, but can be useful if \n",
    "              # burn-in rounds leave the walker distribution too narrow for some reason.\n",
    "              \"disp_floor\": 1e6, \n",
    "              # This is not required, but can be helpful\n",
    "              \"units\": \"solar masses formed\",\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's the description for one parameter.  Let's **build up the rest of our model** as a dictionary of these parameter descriptions, keyed by parameter name.  The type of model you build will depend on your data, the type of object you are looking at,  and your scientific question. For this data, at a minimum we will need some sort of distance or redshift information (which in this example is a fixed parameter), and something descibing the SFH.  We could also add parameters controlling metallicity, dust attenuation and emission, nebular emission, even the IMF. Note that any parameter whose value is not explicitly specified via a model parameter dictionary will be given the default value from python-FSPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we could keep adding parameters by hand, the easiest way to build up this model is to start with some predefined parameter sets from the `prospect.models.templates` module, and then modify them to suit our needs. First, lets look at what pre-packaged parameter sets are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prospect.models.templates import TemplateLibrary\n",
    "\n",
    "# Look at all the prepackaged parameter sets\n",
    "TemplateLibrary.show_contents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the `\"parametric_sfh\"` parameter set will do most of what we want.  Let's look at it in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TemplateLibrary.describe(\"parametric_sfh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good, this model has 5 free parameters:\n",
    "- stellar mass *formed* $M_\\star$,\n",
    "- metallicity $\\log Z/Z_\\odot$, \n",
    "- age $t$ of the galaxy \n",
    "- star formation timescale $\\tau$ for an exponentially declining star formation history (SFH), and\n",
    "- dust attenuation optical depth for a foreground screen, $A_V$.\n",
    "\n",
    "Everything else here is fixed explicitly (e.g. Chabrier IMF via `imf_type:2`).  There are many other `sps` parameters that are set implicitly in the FSPS defaults.  See the python-FSPS documentation for a complete list and details on the default values.\n",
    "\n",
    "Note that by default the stellar mass here refers to the stellar mass *formed* by the given age, which will always be slightly higher than the *surviving* stellar mass, due to mass loss during stellar evolution (winds, SNe, etc.)\n",
    "\n",
    "To tailor this model to our data we'll want to just adjust a couple of the initial values and priors, and add a parameter that will set the distance even though the redshift of theses objects is zero (remember we used absolute magnitudes - these particular very nearby objects are decoupled from the Hubble flow and have very small redshifts, or even blueshifts).\n",
    "\n",
    "However, as for the ``obs`` dictionary we will do all this model building inside a method called ``build_model``.\n",
    "This is useful because then we can control (at run-time) how the model is built via more meta-parameters.  For example the redshift of the model can be specified, or we can decide to change a prior depending on which object the model is fitting. Let's do an example where we change whether the metallicity is a free parameter of the fit or fixed to a particular value, optionally turn on dust emission in the model, and where we can set the redshift by hand as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(object_redshift=None, ldist=10.0, fixed_metallicity=None, add_duste=False, \n",
    "                **extras):\n",
    "    \"\"\"Build a prospect.models.SedModel object\n",
    "    \n",
    "    :param object_redshift: (optional, default: None)\n",
    "        If given, produce spectra and observed frame photometry appropriate \n",
    "        for this redshift. Otherwise, the redshift will be zero.\n",
    "        \n",
    "    :param ldist: (optional, default: 10)\n",
    "        The luminosity distance (in Mpc) for the model.  Spectra and observed \n",
    "        frame (apparent) photometry will be appropriate for this luminosity distance.\n",
    "        \n",
    "    :param fixed_metallicity: (optional, default: None)\n",
    "        If given, fix the model metallicity (:math:`log(Z/Z_sun)`) to the given value.\n",
    "        \n",
    "    :param add_duste: (optional, default: False)\n",
    "        If `True`, add dust emission and associated (fixed) parameters to the model.\n",
    "        \n",
    "    :returns model:\n",
    "        An instance of prospect.models.SedModel\n",
    "    \"\"\"\n",
    "    from prospect.models.sedmodel import SedModel\n",
    "    from prospect.models.templates import TemplateLibrary\n",
    "    from prospect.models import priors\n",
    "\n",
    "    # Get (a copy of) one of the prepackaged model set dictionaries.\n",
    "    # This is, somewhat confusingly, a dictionary of dictionaries, keyed by parameter name\n",
    "    model_params = TemplateLibrary[\"parametric_sfh\"]\n",
    "    \n",
    "   # Now add the lumdist parameter by hand as another entry in the dictionary.\n",
    "   # This will control the distance since we are setting the redshift to zero.  \n",
    "   # In `build_obs` above we used a distance of 10Mpc to convert from absolute to apparent magnitudes, \n",
    "   # so we use that here too, since the `maggies` are appropriate for that distance.\n",
    "    model_params[\"lumdist\"] = {\"N\": 1, \"isfree\": False, \"init\": ldist, \"units\":\"Mpc\"}\n",
    "    \n",
    "    # Let's make some changes to initial values appropriate for our objects and data\n",
    "    model_params[\"zred\"][\"init\"] = 0.0\n",
    "    model_params[\"dust2\"][\"init\"] = 0.05\n",
    "    model_params[\"logzsol\"][\"init\"] = -0.5\n",
    "    model_params[\"tage\"][\"init\"] = 13.\n",
    "    model_params[\"mass\"][\"init\"] = 1e8\n",
    "    \n",
    "    # These are dwarf galaxies, so lets also adjust the metallicity prior,\n",
    "    # the tau parameter upward, and the mass prior downward\n",
    "    model_params[\"dust2\"][\"prior\"] = priors.TopHat(mini=0.0, maxi=2.0)\n",
    "    model_params[\"tau\"][\"prior\"] = priors.LogUniform(mini=1e-1, maxi=1e2)\n",
    "    model_params[\"mass\"][\"prior\"] = priors.LogUniform(mini=1e6, maxi=1e10)\n",
    "\n",
    "    # If we are going to be using emcee, it is useful to provide a \n",
    "    # minimum scale for the cloud of walkers (the default is 0.1)\n",
    "    model_params[\"mass\"][\"disp_floor\"] = 1e6\n",
    "    model_params[\"tau\"][\"disp_floor\"] = 1.0\n",
    "    model_params[\"tage\"][\"disp_floor\"] = 1.0\n",
    "    \n",
    "    # Change the model parameter specifications based on some keyword arguments\n",
    "    if fixed_metallicity is not None:\n",
    "        # make it a fixed parameter\n",
    "        model_params[\"logzsol\"][\"isfree\"] = False\n",
    "        #And use value supplied by fixed_metallicity keyword\n",
    "        model_params[\"logzsol\"]['init'] = fixed_metallicity \n",
    "\n",
    "    if object_redshift is not None:\n",
    "        # make sure zred is fixed\n",
    "        model_params[\"zred\"]['isfree'] = False\n",
    "        # And set the value to the object_redshift keyword\n",
    "        model_params[\"zred\"]['init'] = object_redshift\n",
    "\n",
    "    if add_duste:\n",
    "        # Add dust emission (with fixed dust SED parameters)\n",
    "        # Since `model_params` is a dictionary of parameter specifications, \n",
    "        # and `TemplateLibrary` returns dictionaries of parameter specifications, \n",
    "        # we can just update `model_params` with the parameters described in the \n",
    "        # pre-packaged `dust_emission` parameter set.\n",
    "        model_params.update(TemplateLibrary[\"dust_emission\"])\n",
    "        \n",
    "    # Now instantiate the model object using this dictionary of parameter specifications\n",
    "    model = SedModel(model_params)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "run_params[\"object_redshift\"] = object_redshift\n",
    "run_params[\"fixed_metallicity\"] = None\n",
    "run_params[\"add_duste\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the model using this function and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(**run_params)\n",
    "print(model)\n",
    "print(\"\\nInitial free parameter vector theta:\\n  {}\\n\".format(model.theta))\n",
    "print(\"Initial parameter dictionary:\\n{}\".format(model.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The source object: `build_sps()`\n",
    "\n",
    "We are almost there, but we need an object that will build SEDs for a given set of parameters.  In *prospector* we call these *sps* objects.  Given a dictionary of parameters (provided by the model object), they must be able to return a spectrum and photometry -- corresponding to the data in the `obs` dictionary -- and maybe some ancillary information.  The generation of spectra and photometry is often done using large spectral libraries and, for galaxies, isochrone information.  Typically in *prospector* we use `fsps.StellarPopulation` objects, under thin wrappers that add a little functionality and change the API a bit. The different wrappers correspond to different SFH parameterizations. Here we use `CSPSpecBasis` which works with (linear combinations of) composite stellar populations,  as described in the FSPS manual with `sfh_type` of 1, 4, or 5.  Other `sps` objects can be used for non-parameteric SFH, notably `prospect.sources.FastStepBasis`.\n",
    "\n",
    "Again, it can be helpful to encapsulate the loading of the sps object in a `load_sps` method, with meta-parameters controlling the how the object is instantiated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sps(zcontinuous=1, **extras):\n",
    "    \"\"\"\n",
    "    :param zcontinuous: \n",
    "        A vlue of 1 insures that we use interpolation between SSPs to \n",
    "        have a continuous metallicity parameter (`logzsol`)\n",
    "        See python-FSPS documentation for details\n",
    "    \"\"\"\n",
    "    from prospect.sources import CSPSpecBasis\n",
    "    sps = CSPSpecBasis(zcontinuous=zcontinuous)\n",
    "    return sps\n",
    "\n",
    "run_params[\"zcontinuous\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object often has a large memory footprint, since it contains all the spectral libraries and isochrones.\n",
    "It has one very important method, `get_spectrum()`.  Indeed, the only definition of an `sps` object (for example if you want to make your own) is that it have this method.  In typical use, any FSPS parameter can be passed to this method as an extra keyword argument and the model will be built using that parameter.  Several additional parameters controlling, e.g., spectral smoothing or the wavelength calibration can also be passed to the default objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sps = build_sps(**run_params)\n",
    "help(sps.get_spectrum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the model\n",
    "Now that we have the `sps` object we can also generate a prediction for the data from any set of model parameters.  To see how this works, lets make an SED and plot it!  This will use the initial parameter values for the model we built before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that creating a new model with FSPS is somewhat time-intensive (of order seconds), but once the relevant SSPs have been built they are subsequently stored in cache so similar models can be generated much more quickly (of order milliseconds, unless you are changing parameters that affect the SSPs, like the IMF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the model SED at the initial value of theta\n",
    "theta = model.theta.copy()\n",
    "initial_spec, initial_phot, initial_mfrac = model.sed(theta, obs=obs, sps=sps)\n",
    "title_text = ','.join([\"{}={}\".format(p, model.params[p][0]) \n",
    "                       for p in model.free_params])\n",
    "\n",
    "a = 1.0 + model.params.get('zred', 0.0) # cosmological redshifting\n",
    "# photometric effective wavelengths\n",
    "wphot = obs[\"phot_wave\"]\n",
    "# spectroscopic wavelengths\n",
    "if obs[\"wavelength\"] is None:\n",
    "    # *restframe* spectral wavelengths, since obs[\"wavelength\"] is None\n",
    "    wspec = sps.wavelengths\n",
    "    wspec *= a #redshift them\n",
    "else:\n",
    "    wspec = obs[\"wavelength\"]\n",
    "\n",
    "# establish bounds\n",
    "xmin, xmax = np.min(wphot)*0.8, np.max(wphot)/0.8\n",
    "temp = np.interp(np.linspace(xmin,xmax,10000), wspec, initial_spec)\n",
    "ymin, ymax = temp.min()*0.8, temp.max()/0.4\n",
    "figure(figsize=(16,8))\n",
    "\n",
    "# plot model + data\n",
    "loglog(wspec, initial_spec, label='Model spectrum', \n",
    "       lw=0.7, color='navy', alpha=0.7)\n",
    "errorbar(wphot, initial_phot, label='Model photometry', \n",
    "         marker='s',markersize=10, alpha=0.8, ls='', lw=3,\n",
    "         markerfacecolor='none', markeredgecolor='blue', \n",
    "         markeredgewidth=3)\n",
    "errorbar(wphot, obs['maggies'], yerr=obs['maggies_unc'], \n",
    "         label='Observed photometry',\n",
    "         marker='o', markersize=10, alpha=0.8, ls='', lw=3,\n",
    "         ecolor='red', markerfacecolor='none', markeredgecolor='red', \n",
    "         markeredgewidth=3)\n",
    "title(title_text)\n",
    "\n",
    "# plot Filters\n",
    "for f in obs['filters']:\n",
    "    w, t = f.wavelength.copy(), f.transmission.copy()\n",
    "    t = t / t.max()\n",
    "    t = 10**(0.2*(np.log10(ymax/ymin)))*t * ymin\n",
    "    loglog(w, t, lw=3, color='gray', alpha=0.7)\n",
    "\n",
    "# prettify\n",
    "xlabel('Wavelength [A]')\n",
    "ylabel('Flux Density [maggies]')\n",
    "xlim([xmin, xmax])\n",
    "ylim([ymin, ymax])\n",
    "legend(loc='best', fontsize=20)\n",
    "tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood function\n",
    "Now all we are missing is a likelihood function.  In most cases, this will simply be a function of the **spectral likelihood** and a **photometric likelihood** such that\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = f(\\mathcal{L}_{\\textrm{spec}}, \\mathcal{L}_{\\textrm{phot}}) \\quad .\n",
    "$$\n",
    "\n",
    "Assuming our errors are Normal (i.e. Gaussian), the log-likelihoods for each component are extremely straightforward to define and can be imported directly from *prospector*.  How we choose to combine these likelihoods might vary depending on the particulars of our data. For the demo, our likelihood function for our model parameters $\\boldsymbol{\\theta}$ is just\n",
    "\n",
    "$$\n",
    "\\ln\\mathcal{L}(\\boldsymbol{\\theta}) = \\ln\\mathcal{L}_{\\textrm{spec}}(\\boldsymbol{\\theta}) + \\ln\\mathcal{L}_{\\textrm{phot}}(\\boldsymbol{\\theta}) \\quad .\n",
    "$$\n",
    "\n",
    "Below is a simple version of the likelihood function used in *prospector*.  Note that more complicated likelihoods including covariant noise and fitted noise parameters are possible, using special NoiseModel classes within *prospector*.\n",
    "\n",
    "For nested sampling `lnprobfn(theta, nested=True)` will return the likelihood (since the prior probability is accounted for by drawing proposals from the priors), while for other types of MCMC sampling `lnprobfn(theta, nested=False)` returns the posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prospect.likelihood import lnlike_spec, lnlike_phot, write_log\n",
    "\n",
    "verbose = False\n",
    "def lnprobfn(theta, model=None, obs=None, sps=None, \n",
    "             nested=False, verbose=verbose):\n",
    "    \"\"\"\n",
    "    Given a parameter vector, a model, a dictionary of observational \n",
    "    data, and an sps object, return the ln of the posterior. \n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate prior probability and exit if not within prior\n",
    "    # Also if doing nested sampling, do not include the basic priors, \n",
    "    # since the drawing method includes the prior probability\n",
    "    lnp_prior = model.prior_product(theta, nested=nested)\n",
    "    if not np.isfinite(lnp_prior):\n",
    "        return -np.infty\n",
    "        \n",
    "    # Generate \"mean\" model\n",
    "    spec, phot, mfrac = model.mean_model(theta, obs, sps=sps)\n",
    " \n",
    "    # Calculate likelihoods\n",
    "    lnp_spec = lnlike_spec(spec, obs=obs)\n",
    "    lnp_phot = lnlike_phot(phot, obs=obs)\n",
    "\n",
    "    return lnp_prior + lnp_phot + lnp_spec\n",
    "\n",
    "run_params[\"verbose\"] = verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can also be useful for some optimization methods (i.e. Levenberg-Marquardt) to define a function that returns the vector of chi-square residuals.  Let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prospect.likelihood import chi_spec, chi_phot\n",
    "def chivecfn(theta):\n",
    "    \"\"\"A version of lnprobfn that returns the simple uncertainty \n",
    "    normalized residual instead of the log-posterior, for use with \n",
    "    least-squares optimization methods like Levenburg-Marquardt.\n",
    "    \n",
    "    It's important to note that the returned chi vector does not \n",
    "    include the prior probability.\n",
    "    \"\"\"\n",
    "    lnp_prior = model.prior_product(theta)\n",
    "    if not np.isfinite(lnp_prior):\n",
    "        return np.zeros(model.ndim) - np.infty\n",
    "\n",
    "    # Generate mean model\n",
    "    try:\n",
    "        spec, phot, x = model.mean_model(theta, obs, sps=sps)\n",
    "    except(ValueError):\n",
    "        return np.zeros(model.ndim) - np.infty\n",
    "\n",
    "    chispec = chi_spec(spec, obs)\n",
    "    chiphot = chi_phot(phot, obs)\n",
    "    return np.concatenate([chispec, chiphot])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, *prospector* comes with a pre-built `lnprobfn` that can incorporate more complex noise models, and can be either return ln-probability or chi-square or a vector of residuals depending on the algorithm being used.  So let's use that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prospect.fitting import lnprobfn\n",
    "help(lnprobfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Prospector\n",
    "Now that we have defined the model and set up the data that we want to fit, we are ready to run *prospector*.  We will do this in a few steps.  First we will run all the convenience functions we made earlier to get the fitting ingredients and set up the output.  Then we will fit the model using 1) $\\chi^2$ minimization; 2) ensemble MCMC sampler around the best location from the minimization 3) dynamic nested sampling with `dynesty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will run all our building functions\n",
    "obs = build_obs(attrs, spectr,**run_params)\n",
    "sps = build_sps(**run_params)\n",
    "model = build_model(**run_params)\n",
    "\n",
    "# For fsps based sources it is useful to \n",
    "# know which stellar isochrone and spectral library\n",
    "# we are using\n",
    "print(sps.ssp.libraries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use, appropriately enough, the `fit_model` method (with different options) for each of the three different ways of fitting the model described above.  Let's take a look at this method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prospect.fitting import fit_model\n",
    "help(fit_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note in particular that the output is a dictionary with *both* optimization and sampling entries.  This is because one can do both within a single call to `fit_model`, and for ensemble sampling in particular it can be useful to do optimization first to get close to the data before beginnning the sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimization\n",
    "We can attempt to initialize our model reasonably close to the data by using some numerical minimization routines.\n",
    "Here we will use Levenberg-Marquardt. Keywords that control the optimization algorithm will again be stored in the `run_params` dictionary. Levenberg-Marquardt requires a likelihood function that returns a vector of chi values, not an actual likelihood, but the `lnprobfn` we imported supports that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- start minimization ----\n",
    "run_params[\"dynesty\"] = False\n",
    "run_params[\"emcee\"] = False\n",
    "run_params[\"optimize\"] = True\n",
    "run_params[\"min_method\"] = 'lm'\n",
    "# We'll start minimization from \"nmin\" separate places, \n",
    "# the first based on the current values of each parameter and the \n",
    "# rest drawn from the prior.  Starting from these extra draws \n",
    "# can guard against local minima, or problems caused by \n",
    "# starting at the edge of a prior (e.g. dust2=0.0)\n",
    "run_params[\"nmin\"] = 2\n",
    "\n",
    "output = fit_model(obs, model, sps, lnprobfn=lnprobfn, **run_params)\n",
    "\n",
    "print(\"Done optmization in {}s\".format(output[\"optimization\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View model after minimization\n",
    "Now let's see how our model looks in the data space after minimization. The model should already be set to the result of the best optimization, but we can also find the best run and set the model parameters to that result by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.theta)\n",
    "(results, topt) = output[\"optimization\"]\n",
    "# Find which of the minimizations gave the best result, \n",
    "# and use the parameter vector for that minimization\n",
    "ind_best = np.argmin([r.cost for r in results])\n",
    "print(ind_best)\n",
    "theta_best = results[ind_best].x.copy()\n",
    "print(theta_best)\n",
    "\n",
    "# generate model\n",
    "prediction = model.mean_model(theta_best, obs=obs, sps=sps)\n",
    "pspec, pphot, pfrac = prediction\n",
    "\n",
    "figure(figsize=(16,8))\n",
    "\n",
    "# plot Data, best fit model, and old models\n",
    "loglog(wspec, initial_spec, label='Old model spectrum',\n",
    "       lw=0.7, color='gray', alpha=0.5)\n",
    "errorbar(wphot, initial_phot, label='Old model Photometry', \n",
    "         marker='s', markersize=10, alpha=0.6, ls='', lw=3, \n",
    "         markerfacecolor='none', markeredgecolor='gray', \n",
    "         markeredgewidth=3)\n",
    "loglog(wspec, pspec, label='Model spectrum', \n",
    "       lw=0.7, color='slateblue', alpha=0.7)\n",
    "errorbar(wphot, pphot, label='Model photometry', \n",
    "         marker='s', markersize=10, alpha=0.8, ls='', lw=3,\n",
    "         markerfacecolor='none', markeredgecolor='slateblue', \n",
    "         markeredgewidth=3)\n",
    "errorbar(wphot, obs['maggies'], yerr=obs['maggies_unc'],\n",
    "         label='Observed photometry', \n",
    "         marker='o', markersize=10, alpha=0.8, ls='', lw=3, \n",
    "         ecolor='tomato', markerfacecolor='none', markeredgecolor='tomato', \n",
    "         markeredgewidth=3)\n",
    "\n",
    "# plot filter transmission curves\n",
    "for f in obs['filters']:\n",
    "    w, t = f.wavelength.copy(), f.transmission.copy()\n",
    "    t = t / t.max()\n",
    "    t = 10**(0.2*(np.log10(ymax/ymin)))*t * ymin\n",
    "    loglog(w, t, lw=3, color='gray', alpha=0.7)\n",
    "\n",
    "# Prettify\n",
    "xlabel('Wavelength [A]')\n",
    "ylabel('Flux Density [maggies]')\n",
    "xlim([xmin, xmax])\n",
    "ylim([ymin, ymax])\n",
    "legend(loc='best', fontsize=20)\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should look much better, except maybe for the filters that we have masked out.  You can also run the minimization cell multiple times to see if the fit gets better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the Posterior: Ensemble sampling\n",
    "Now that we're somewhat burned in, we can begin sampling from the posterior using **Markov Chain Monte Carlo** (MCMC).  For this we will use the ensemble sampling agorithms in *emcee*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm used in *emcee* requires several options to be specified, related to the number of walkers, the number of iterations, and to rounds of burn-in.  For convenience we will store these in the `run_params` meta-parameter dictionary that we've been using. Sampling will begin with a cloud of walkers initialized around the current `model.theta` value, but optionally another round of optmization can be done before sampling begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to False if you don't want to do another optimization\n",
    "# before emcee sampling (but note that the \"optimization\" entry \n",
    "# in the output dictionary will be (None, 0.) in this case)\n",
    "# If set to true then another round of optmization will be performed \n",
    "# before sampling begins and the \"optmization\" entry of the output\n",
    "# will be populated.\n",
    "run_params[\"optimize\"] = False\n",
    "run_params[\"emcee\"] = True\n",
    "run_params[\"dynesty\"] = False\n",
    "# Number of emcee walkers\n",
    "run_params[\"nwalkers\"] = 128\n",
    "# Number of iterations of the MCMC sampling\n",
    "run_params[\"niter\"] = 512\n",
    "# Number of iterations in each round of burn-in\n",
    "# After each round, the walkers are reinitialized based on the \n",
    "# locations of the highest probablity half of the walkers.\n",
    "run_params[\"nburn\"] = [16, 32, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and start sampling! (This should take of order 10 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = fit_model(obs, model, sps, lnprobfn=lnprobfn, **run_params)\n",
    "print('done emcee in {0}s'.format(output[\"sampling\"][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Output\n",
    "Now that everything's all set, let's save our results to disk.  These will be written to an hdf5 file with the given name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prospect.io import write_results as writer\n",
    "hfile = \"demo_emcee_mcmc_\" + output_tag + \".h5\"\n",
    "writer.write_hdf5(hfile, run_params, model, obs,\n",
    "                  output[\"sampling\"][0], output[\"optimization\"][0],\n",
    "                  tsample=output[\"sampling\"][1],\n",
    "                  toptimize=output[\"optimization\"][1])\n",
    "\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the Posterior: Nested sampling\n",
    "Instead of ensemble MCMC sampling we can sample using dynamic nested sampling via *dynesty*.  Dynamic nested sampling works by sampling from within successive iso-likelihood contours, transforming a distribution of live points from the prior to the posterior.  It does not benefit from burn-in via optimization.\n",
    "\n",
    "There are a number of parameters that control the operation of *dynesty*.  A listing and brief description of each is given in the default *prospector* arguments (see below), here we will set just a few of the most important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params[\"dynesty\"] = True\n",
    "run_params[\"optmization\"] = False\n",
    "run_params[\"emcee\"] = False\n",
    "run_params[\"nested_method\"] = \"rwalk\"\n",
    "run_params[\"nlive_init\"] = 400\n",
    "run_params[\"nlive_batch\"] = 200\n",
    "run_params[\"nested_dlogz_init\"] = 0.05\n",
    "run_params[\"nested_posterior_thresh\"] = 0.05\n",
    "run_params[\"nested_maxcall\"] = int(1e7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = fit_model(obs, model, sps, lnprobfn=lnprobfn, **run_params)\n",
    "print('done dynesty in {0}s'.format(output[\"sampling\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prospect.io import write_results as writer\n",
    "hfile = \"demo_dynesty_mcmc_\" +  output_tag+ \".h5\"\n",
    "writer.write_hdf5(hfile, run_params, model, obs,\n",
    "                  output[\"sampling\"][0], output[\"optimization\"][0],\n",
    "                  tsample=output[\"sampling\"][1],\n",
    "                  toptimize=output[\"optimization\"][1])\n",
    "\n",
    "print('Finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Command-line operation\n",
    "\n",
    "The preferred way to use prospector is through command line operation.  This is difficult to demonstrate in a notebook, but essentially one would place the `build_obs`, `build_model`, and `build_sps` methods in a single **parameter file**.  Then, in the `__main__` portion of the **parameter file** the entries of the `run_params` dictionary would be set via command-line arguments (via `argparse`), the `build_*` methods would be run, and `fit_model` would be called. For an example please see `demo_params.py` and `demo_mock_params.py` in the prospector demo directory.\n",
    "\n",
    "A number of options used by the code are available in a prebuilt argument parser, which can be augmented with anything necessary for the `build_*` methods that you create.  Below is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prospect import prospect_args\n",
    "# - Parser with default arguments -\n",
    "parser = prospect_args.get_parser()\n",
    "# - Add custom arguments -\n",
    "parser.add_argument('--add_duste', action=\"store_true\",\n",
    "                    help=\"If set, add dust emission to the model.\")\n",
    "parser.add_argument('--ldist', type=float, default=10,\n",
    "                    help=(\"Luminosity distance in Mpc. Defaults to 10\"\n",
    "                          \"(for case of absolute mags)\"))\n",
    "args, _ = parser.parse_known_args()\n",
    "cli_run_params = vars(args)\n",
    "print(cli_run_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Results\n",
    "There are a few basic plotting tools available to do a quick check on the results available in `prospect.io.read_results` and `prospect.utils.plotting`. But first we need to read the output files we made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading output files\n",
    "Reading our results from our HDF5 file is straightforward using the `results_from` method.  This returns a \"results\" dictionary, the `obs` dictionary of data to which the model was fit, and in some situations the `SedModel` object that was used in the fitting.\n",
    "\n",
    "In this example we will look at the *emcee* results file that we created, but you can do the same for the *dynesty* results file.  The main difference is that the chains for each fit type have a different dimensionality, and that the samples in the *dynesty* chains come with associated **weights**.  The weights and different shapes are handled automatically by the plotting functions included in *prospector*, but custom plotting code should take care of the difference.\n",
    "\n",
    "In this notebook we have already defined the `model` (and `sps`) object, but in general one will want to recreate it.  *Prospector* allows you to do this by storing the `build_model` and `build_sps` code as text in the HDF5 file -- this is possible with the use of *parameter files* and command line operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import prospect.io.read_results as reader\n",
    "results_type = \"emcee\" # | \"dynesty\"\n",
    "# grab results (dictionary), the obs dictionary, and our corresponding models\n",
    "# When using parameter files set `dangerous=True`\n",
    "result, obs, _ = reader.results_from(\"demo_{}_mcmc_{}.h5\".format(results_type,output_tag), dangerous=False)\n",
    "\n",
    "#The following commented lines reconstruct the model and sps object, \n",
    "# if a parameter file continaing the `build_*` methods was saved along with the results\n",
    "#model = reader.get_model(result)\n",
    "#sps = reader.get_sps(result)\n",
    "\n",
    "# let's look at what's stored in the `result` dictionary\n",
    "print(result.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting parameter traces\n",
    "To see how our MCMC samples look, we can examine a few traces (that is, the evolution of the parameter value with iteration in the MCMC chain.)  You can use these plots (and the chains more generally) to assess whether the MCMC has converged, or if you need to sample for more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if results_type == \"emcee\":\n",
    "    chosen = np.random.choice(result[\"run_params\"][\"nwalkers\"], size=10, replace=False)\n",
    "    tracefig = reader.traceplot(result, figsize=(20,10), chains=chosen)\n",
    "else:\n",
    "    tracefig = reader.traceplot(result, figsize=(20,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a corner plot\n",
    "Our samples more generally can be shown using a corner/triangle plot.  The `subtriangle()` method below is a very thin wrapper on Dan Foreman-Mackey's **corner.py** code.  We'll overplot the MAP value as blue lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum a posteriori (of the locations visited by the MCMC sampler)\n",
    "imax = np.argmax(result['lnprobability'])\n",
    "if results_type == \"emcee\":\n",
    "    i, j = np.unravel_index(imax, result['lnprobability'].shape)\n",
    "    theta_max = result['chain'][i, j, :].copy()\n",
    "    thin = 5\n",
    "else:\n",
    "    theta_max = result[\"chain\"][imax, :]\n",
    "    thin = 1\n",
    "\n",
    "print('Optimization value: {}'.format(theta_best))\n",
    "print('MAP value: {}'.format(theta_max))\n",
    "cornerfig = reader.subcorner(result, start=0, thin=thin, truths=theta_best, \n",
    "                             fig=subplots(5,5,figsize=(27,27))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at SEDs and residuals\n",
    "Finally, let's just take a look at a random model drawn from our chains, and at the highest posterior probability model in the chain.  In this notebook we already have the `sps` object instantiated, but in general we may have to regenerate it based on information stored in the output file using the `prospect.io.read_results.get_sps` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly chosen parameters from chain\n",
    "randint = np.random.randint\n",
    "if results_type == \"emcee\":\n",
    "    nwalkers, niter = run_params['nwalkers'], run_params['niter']\n",
    "    theta = result['chain'][randint(nwalkers), randint(niter)]\n",
    "else:\n",
    "    theta = result[\"chain\"][randint(len(result[\"chain\"]))]\n",
    "\n",
    "# generate models\n",
    "# sps = reader.get_sps(result)  # this works if using parameter files\n",
    "mspec, mphot, mextra = model.mean_model(theta, obs, sps=sps)\n",
    "mspec_map, mphot_map, _ = model.mean_model(theta_max, obs, sps=sps)\n",
    "\n",
    "# Make plot of data and model\n",
    "figure(figsize=(16,8))\n",
    "\n",
    "loglog(wspec, mspec, label='Model spectrum (random draw)',\n",
    "       lw=0.7, color='navy', alpha=0.7)\n",
    "loglog(wspec, mspec_map, label='Model spectrum (MAP)',\n",
    "       lw=0.7, color='green', alpha=0.7)\n",
    "errorbar(wphot, mphot, label='Model photometry (random draw)',\n",
    "         marker='s', markersize=10, alpha=0.8, ls='', lw=3, \n",
    "         markerfacecolor='none', markeredgecolor='blue', \n",
    "         markeredgewidth=3)\n",
    "errorbar(wphot, mphot_map, label='Model photometry (MAP)',\n",
    "         marker='s', markersize=10, alpha=0.8, ls='', lw=3, \n",
    "         markerfacecolor='none', markeredgecolor='green', \n",
    "         markeredgewidth=3)\n",
    "errorbar(wphot, obs['maggies'], yerr=obs['maggies_unc'], \n",
    "         label='Observed photometry', ecolor='red', \n",
    "         marker='o', markersize=10, ls='', lw=3, alpha=0.8, \n",
    "         markerfacecolor='none', markeredgecolor='red', \n",
    "         markeredgewidth=3)\n",
    "\n",
    "# plot transmission curves\n",
    "for f in obs['filters']:\n",
    "    w, t = f.wavelength.copy(), f.transmission.copy()\n",
    "    t = t / t.max()\n",
    "    t = 10**(0.2*(np.log10(ymax/ymin)))*t * ymin\n",
    "    loglog(w, t, lw=3, color='gray', alpha=0.7)\n",
    "\n",
    "xlabel('Wavelength [A]')\n",
    "ylabel('Flux Density [maggies]')\n",
    "xlim([xmin, xmax])\n",
    "ylim([ymin, ymax])\n",
    "legend(loc='best', fontsize=20)\n",
    "tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "prospector",
   "language": "python",
   "name": "prospector"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "nav_menu": {
    "height": "413px",
    "width": "290px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
